[{"content":"Introduction\rThe FIFA World Cup 2022 Final between Argentina and France will be remembered as one of the most thrilling matches in football history. With Argentina clinching victory in a penalty shootout after a 3-3 draw, this match provided a wealth of tactical, individual, and team insights. This analysis working with only Argentina\u0026rsquo;s player passing data to find out key patterns, tactics and insight that shaped this epic encounter.\nLibreries and Data Import\rTo conduct this analysis, we utilized the StatsBomb API for detailed event data. Key Python libraries included:\npandas for data manipulation, mplsoccer for visualization, matplotlib for plotting, and scipy for data smoothing. The dataset included detailed events such as passes, shots, fouls, and defensive actions from the final matc 1 2 3 4 5 6 7 # Import Libreries \u0026amp; Packeges import pandas as pd import matplotlib.pyplot as plt from mplsoccer import Pitch, VerticalPitch, FontManager from matplotlib import rcParams from statsbombpy import sb from scipy.ndimage import gaussian_filter 1 2 3 # Call Statsbomb API to import data matches = sb.matches(competition_id=43, season_id=106) matches[matches[\u0026#39;competition_stage\u0026#39;]==\u0026#39;Final\u0026#39;] match_id\rmatch_date\rkick_off\rcompetition\rseason\rhome_team\raway_team\rhome_score\raway_score\rmatch_status\r...\rlast_updated_360\rmatch_week\rcompetition_stage\rstadium\rreferee\rhome_managers\raway_managers\rdata_version\rshot_fidelity_version\rxy_fidelity_version\r9\r3869685\r2022-12-18\r17:00:00.000\rInternational - FIFA World Cup\r2022\rArgentina\rFrance\r3\r3\ravailable\r...\r2023-08-17T15:55:15.164685\r7\rFinal\rLusail Stadium\rSzymon Marciniak\rLionel Sebasti√°n Scaloni\rDidier Deschamps\r1.1.0\r2\r2\r1 rows √ó 22 columns\n1 2 # Select Fifa Worldcup 2022 \u0026#34;Argentina vs France\u0026#34; final match events = sb.events(match_id=3869685) Data Preprocessing\rBefore diving into analysis, preprocessing steps involved filtering and structuring the data:\nExtracting key events (passes, shot,, goals, player positions), Filtering player-specific data (Messi, Argentina), Data Normalization 1 2 # Check the data events.tail() 50_50\rbad_behaviour_card\rball_receipt_outcome\rball_recovery_offensive\rball_recovery_recovery_failure\rblock_deflection\rblock_offensive\rcarry_end_location\rclearance_aerial_won\rclearance_body_part\r...\rsubstitution_outcome\rsubstitution_outcome_id\rsubstitution_replacement\rsubstitution_replacement_id\rtactics\rteam\rteam_id\rtimestamp\rtype\runder_pressure\r4402\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\rNaN\rNaN\rNaN\rNaN\rNaN\rArgentina\r779\r00:19:07.472\rHalf End\rNaN\r4403\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\rNaN\rNaN\rNaN\rNaN\rNaN\rFrance\r771\r00:05:58.866\rHalf End\rNaN\r4404\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\rNaN\rNaN\rNaN\rNaN\rNaN\rArgentina\r779\r00:05:58.866\rHalf End\rNaN\r4405\rNaN\rYellow Card\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\rNaN\rNaN\rNaN\rNaN\rNaN\rFrance\r771\r00:49:35.193\rBad Behaviour\rNaN\r4406\rNaN\rYellow Card\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\rNaN\r...\rNaN\rNaN\rNaN\rNaN\rNaN\rArgentina\r779\r00:04:57.514\rBad Behaviour\rNaN\r5 rows √ó 94 columns\n1 2 # Check the available colomns events.columns Index(['50_50', 'bad_behaviour_card', 'ball_receipt_outcome',\r'ball_recovery_offensive', 'ball_recovery_recovery_failure',\r'block_deflection', 'block_offensive', 'carry_end_location',\r'clearance_aerial_won', 'clearance_body_part', 'clearance_head',\r'clearance_left_foot', 'clearance_other', 'clearance_right_foot',\r'counterpress', 'dribble_nutmeg', 'dribble_outcome', 'dribble_overrun',\r'duel_outcome', 'duel_type', 'duration', 'foul_committed_advantage',\r'foul_committed_card', 'foul_committed_offensive',\r'foul_committed_penalty', 'foul_committed_type', 'foul_won_advantage',\r'foul_won_defensive', 'foul_won_penalty', 'goalkeeper_body_part',\r'goalkeeper_end_location', 'goalkeeper_outcome', 'goalkeeper_position',\r'goalkeeper_technique', 'goalkeeper_type', 'id', 'index',\r'interception_outcome', 'location', 'match_id', 'minute', 'off_camera',\r'out', 'pass_aerial_won', 'pass_angle', 'pass_assisted_shot_id',\r'pass_body_part', 'pass_cross', 'pass_deflected', 'pass_end_location',\r'pass_goal_assist', 'pass_height', 'pass_inswinging', 'pass_length',\r'pass_outcome', 'pass_outswinging', 'pass_recipient',\r'pass_recipient_id', 'pass_shot_assist', 'pass_switch',\r'pass_technique', 'pass_through_ball', 'pass_type', 'period',\r'play_pattern', 'player', 'player_id', 'position', 'possession',\r'possession_team', 'possession_team_id', 'related_events', 'second',\r'shot_aerial_won', 'shot_body_part', 'shot_end_location',\r'shot_first_time', 'shot_freeze_frame', 'shot_key_pass_id',\r'shot_one_on_one', 'shot_outcome', 'shot_statsbomb_xg',\r'shot_technique', 'shot_type', 'substitution_outcome',\r'substitution_outcome_id', 'substitution_replacement',\r'substitution_replacement_id', 'tactics', 'team', 'team_id',\r'timestamp', 'type', 'under_pressure'],\rdtype='object')\r1 2 3 4 5 # Create x and y lables from \u0026#34;location\u0026#34; data events[[\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;]]= events[\u0026#39;location\u0026#39;].apply(pd.Series) # Create end_x and end_y Lables from \u0026#34;pass_end_location\u0026#34; Data events[[\u0026#39;end_x\u0026#39;,\u0026#39;end_y\u0026#39;]]= events[\u0026#39;pass_end_location\u0026#39;].apply(pd.Series) Ploting \u0026amp; Visualization\rAregntina\u0026rsquo;s Pass Analysis\rAnalysing Argentina\u0026rsquo;s passes and player data will give us key insights into the team\u0026rsquo;s build-up play. Here we will try to understand:\nCompleted vs Missed/Other passes Player individual passes Messi\u0026rsquo;s passes pattern Passes leads to goal 1 2 3 4 5 # Create new data frame for all the passe\u0026#39;s by Aregentine players ag_pass_df = events[(events[\u0026#39;type\u0026#39;]==\u0026#39;Pass\u0026#39;) \u0026amp; (events[\u0026#39;team\u0026#39;]==\u0026#39;Argentina\u0026#39;)] # Filter sucessfull passes mask_pass = ag_pass_df[\u0026#39;pass_outcome\u0026#39;].isnull() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Set up the pitch pitch = Pitch(pitch_type=\u0026#39;statsbomb\u0026#39;, pitch_color=\u0026#39;#22312b\u0026#39;, line_color=\u0026#39;#c7d5cc\u0026#39;) fig, axs = pitch.draw(figsize=(15, 13), constrained_layout=True, tight_layout=False) fig.set_facecolor(\u0026#39;#22312b\u0026#39;) # Plot the completed passes pitch.lines(ag_pass_df[mask_pass].x, ag_pass_df[mask_pass].y, ag_pass_df[mask_pass].end_x, ag_pass_df[mask_pass].end_y, lw=5, transparent=True, comet=True, color=\u0026#39;#3cada2\u0026#39;, ax=axs, label=\u0026#39;completed passes\u0026#39;) pitch.scatter(ag_pass_df[mask_pass].x, ag_pass_df[mask_pass].y, alpha = 0.2, color = \u0026#34;#3cada2\u0026#34;, ax=axs) # Plot the other passes pitch.lines(ag_pass_df[~mask_pass].x, ag_pass_df[~mask_pass].y, ag_pass_df[~mask_pass].end_x, ag_pass_df[~mask_pass].end_y, lw=5, transparent=True, comet=True, color=\u0026#39;#ba4f45\u0026#39;, ax=axs, label=\u0026#39;other passes\u0026#39;) pitch.scatter(ag_pass_df[~mask_pass].x, ag_pass_df[~mask_pass].y, alpha = 0.2, color = \u0026#34;#ba4f45\u0026#34;, ax=axs) # Set up the legend axs.legend(facecolor=\u0026#39;#22312b\u0026#39;, handlelength=5, edgecolor=\u0026#39;None\u0026#39;, fontsize=15, loc=\u0026#39;upper left\u0026#39;, labelcolor=\u0026#39;white\u0026#39;) # Set the title ax_title = axs.set_title(\u0026#39;Argentina Passes Against France\u0026#39;, fontsize=20, color=\u0026#39;white\u0026#39;) 1 2 # Create player name list players = ag_pass_df[\u0026#39;player\u0026#39;].unique() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Set up the pitch pitch = Pitch(pitch_type=\u0026#39;statsbomb\u0026#39;, pitch_color=\u0026#39;#22312b\u0026#39;, line_color=\u0026#39;#c7d5cc\u0026#39;) fig, axs = pitch.draw(nrows=5, ncols=4, figsize=(20,15),constrained_layout=True, tight_layout=False) fig.set_facecolor(\u0026#39;#22312b\u0026#39;) # Convert the axs into one dimension axs = axs.flatten() # Plot each player\u0026#39;s passes for i, player in enumerate(players): player_passes = ag_pass_df[ag_pass_df[\u0026#39;player\u0026#39;] == player] pitch.lines(player_passes[mask_pass].x, player_passes[mask_pass].y, player_passes[mask_pass].end_x, player_passes[mask_pass].end_y, lw=2, transparent=True, comet=True, color=\u0026#39;#3cada2\u0026#39;, ax=axs[i], label=\u0026#39;completed passes\u0026#39;) pitch.scatter(player_passes[mask_pass].x, player_passes[mask_pass].y, alpha = 0.2, color = \u0026#34;#3cada2\u0026#34;, ax=axs[i]) axs[i].set_title(f\u0026#34;Passes by {player}\u0026#34;, fontsize=10, color=\u0026#39;white\u0026#39;) # Remove extra pitches for i in range(17,20): axs[i].remove() # Set the title fig.suptitle(\u0026#34;Argentine Player\u0026#39;s Individual Passes\u0026#34;, fontsize = 25, color=\u0026#39;white\u0026#39;) plt.show() 1 2 # Filter all the passes from Messi messi_pass = ag_pass_df[ag_pass_df[\u0026#39;player\u0026#39;]==\u0026#39;Lionel Andr√©s Messi Cuccittini\u0026#39;] 1 2 3 4 5 6 7 8 9 10 11 12 # Set up the pitch pitch = Pitch(pitch_type=\u0026#39;statsbomb\u0026#39;, pitch_color=\u0026#39;#22312b\u0026#39;, line_color=\u0026#39;#c7d5cc\u0026#39;) fig, axs = pitch.draw(figsize=(10, 8), constrained_layout=True, tight_layout=False) fig.set_facecolor(\u0026#39;#22312b\u0026#39;) # Plot the pass convex hull hull = pitch.convexhull(messi_pass.x, messi_pass.y) poly = pitch.polygon(hull, ax=axs, edgecolor=\u0026#39;cornflowerblue\u0026#39;, facecolor=\u0026#39;cornflowerblue\u0026#39;, alpha=0.3) scatter = pitch.scatter(messi_pass.x, messi_pass.y, ax=axs, edgecolor=\u0026#39;black\u0026#39;, facecolor=\u0026#39;cornflowerblue\u0026#39;) # Set the title axs_title = axs.set_title(\u0026#34;Messi\u0026#39;s Pass Convex Hull\u0026#34;, fontsize=20, color=\u0026#39;white\u0026#39;) 1 2 3 4 shot_pass_df = events.loc[(events[\u0026#39;pass_assisted_shot_id\u0026#39;].notnull()) \u0026amp; (events[\u0026#39;team\u0026#39;]==\u0026#39;Argentina\u0026#39;), [\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;end_x\u0026#39;, \u0026#39;end_y\u0026#39;, \u0026#39;pass_assisted_shot_id\u0026#39;]] shot_df = events.loc[(events[\u0026#39;type\u0026#39;]==\u0026#39;Shot\u0026#39;) \u0026amp; (events[\u0026#39;team\u0026#39;]==\u0026#39;Argentina\u0026#39;), [\u0026#39;id\u0026#39;, \u0026#39;shot_outcome\u0026#39;, \u0026#39;shot_statsbomb_xg\u0026#39;, \u0026#39;minute\u0026#39;]].rename({\u0026#39;id\u0026#39;:\u0026#39;pass_assisted_shot_id\u0026#39;}, axis=1) shot_pass_df = shot_pass_df.merge(shot_df, how=\u0026#39;left\u0026#39;) mask_goal = shot_pass_df[\u0026#39;shot_outcome\u0026#39;] == \u0026#39;Goal\u0026#39; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # Setup the pitch pitch = VerticalPitch(pitch_type=\u0026#39;statsbomb\u0026#39;, pitch_color=\u0026#39;#22312b\u0026#39;, line_color=\u0026#39;#c7d5cc\u0026#39;, half=True, pad_top=2) fig, axs = pitch.grid(endnote_height=0.03, endnote_space=0, figheight=7, title_height=0.08, title_space=0, axis=False, grid_height=0.82) fig.set_facecolor(\u0026#39;#22312b\u0026#39;) # Plot the completed passes pitch.lines(shot_pass_df.x, shot_pass_df.y, shot_pass_df.end_x, shot_pass_df.end_y, lw=6, transparent=True, comet=True, cmap=\u0026#39;jet\u0026#39;, label=\u0026#39;pass leading to shot\u0026#39;, ax=axs[\u0026#39;pitch\u0026#39;]) # Plot the goals pitch.scatter(shot_pass_df[mask_goal].end_x, shot_pass_df[mask_goal].end_y, s=300, marker=\u0026#39;football\u0026#39;, edgecolors=\u0026#39;black\u0026#39;, c=\u0026#39;white\u0026#39;, zorder=2, label=\u0026#39;goal\u0026#39;, ax=axs[\u0026#39;pitch\u0026#39;]) pitch.scatter(shot_pass_df[~mask_goal].end_x, shot_pass_df[~mask_goal].end_y, edgecolors=\u0026#39;white\u0026#39;, c=\u0026#39;#22312b\u0026#39;, s=300, zorder=2, label=\u0026#39;shot\u0026#39;, ax=axs[\u0026#39;pitch\u0026#39;]) # Set the title axs[\u0026#39;title\u0026#39;].text(0.5, 0.5, f\u0026#39;Argentina Passes Leading to Shots\u0026#39;, color=\u0026#39;white\u0026#39;, va=\u0026#39;center\u0026#39;, ha=\u0026#39;center\u0026#39;, fontsize=20) # Set legend legend = axs[\u0026#39;pitch\u0026#39;].legend(facecolor=\u0026#39;#22312b\u0026#39;, edgecolor=\u0026#39;None\u0026#39;, loc=\u0026#39;lower center\u0026#39;, handlelength=4) for text in legend.get_texts(): text.set_fontsize(15) text.set_color(\u0026#39;white\u0026#39;) Argentina\u0026rsquo;s Pressure Analysis\rIn this section we will try to understand at what positions argentine players dominated the most and at what level\n1 2 # Filter pressure data mask_presure = (events[\u0026#39;type\u0026#39;] == \u0026#39;Pressure\u0026#39;) \u0026amp; (events[\u0026#39;team\u0026#39;] == \u0026#39;Argentina\u0026#39;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Setup pitch pitch = Pitch(pitch_type=\u0026#39;statsbomb\u0026#39;, line_zorder=2, pitch_color=\u0026#39;#22312b\u0026#39;, line_color=\u0026#39;#efefef\u0026#39;) # Draw fig, axs = pitch.draw(figsize=(9, 7)) fig.set_facecolor(\u0026#39;#22312b\u0026#39;) bin_statistic = pitch.bin_statistic(events[mask_presure].x, events[mask_presure].y, statistic=\u0026#39;count\u0026#39;, bins=(25, 25)) bin_statistic[\u0026#39;statistic\u0026#39;] = gaussian_filter(bin_statistic[\u0026#39;statistic\u0026#39;], 1) pcm = pitch.heatmap(bin_statistic, ax=axs, cmap=\u0026#39;hot\u0026#39;, edgecolors=\u0026#39;#22312b\u0026#39;) # Add the colorbar and format off-white cbar = fig.colorbar(pcm, ax=axs, shrink=0.6) cbar.outline.set_edgecolor(\u0026#39;#efefef\u0026#39;) cbar.ax.yaxis.set_tick_params(color=\u0026#39;#efefef\u0026#39;) ticks = plt.setp(plt.getp(cbar.ax.axes, \u0026#39;yticklabels\u0026#39;), color=\u0026#39;#efefef\u0026#39;) # Set the title axs_title = axs.set_title(\u0026#39;Pressure Applied by Argentina\u0026#39;, fontsize=20, color=\u0026#39;white\u0026#39;) Conclusion\rArgentina\u0026rsquo;s performance in the FIFA World Cup 2022 Final showcased their tactical discipline and cohesive play, particularly evident in their passing structure and defensive pressure.\nPassing Insights: Argentina\u0026rsquo;s passing network revealed a well-balanced approach, effectively utilizing both central and wide areas. The team prioritized ball retention and progressive play, with Lionel Messi orchestrating attacks through key passes. Their passing patterns created numerical superiority in crucial zones, allowing them to control the game\u0026rsquo;s tempo and exploit spaces in France\u0026rsquo;s defense.\nDefensive Pressure: Argentina applied structured defensive pressure, particularly in the first half. Their pressing actions were concentrated in the midfield, aimed at disrupting France‚Äôs build-up play and limiting their access to dangerous areas. By maintaining compact defensive lines, Argentina forced turnovers and quickly transitioned into attack, a strategy that kept France under constant pressure.\nThis combination of controlled passing and well-coordinated pressing was instrumental in Argentina‚Äôs dominance for large parts of the match, ultimately contributing to their historic victory.\n","date":"2024-11-24T00:00:00Z","image":"http://localhost:1313/p/fifa2022-final/cover_hu2251147847425360426.jpg","permalink":"http://localhost:1313/p/fifa2022-final/","title":"How to Analyze Football Data Using Python and StatsBomb: FIFA World Cup 2022 Final Example"},{"content":"One of the great challenges for hospitals, clinics, and other healthcare facilities is to ensure they has the right skilled professionals to meet their need. The healthcare staffing industry plays a vital role in fulfilling this need. However, the the complicated nature of staffing healthcare workers - due to fluctuating demands, varying qualifications, and compliance issues, makes the process highly challenging. To overcome this problem we utilize data analytics that can provide a transformative solution.\nToday I am going to explore how data analytics can be use to revolutionize the healthcare staffing industry, improving efficiency, reducing costs, and ultimately ensuring successful hiring. And share some real life examples from my work experience.\nPredictive Analytics for Staffing Demand\rUsing predictive analytics is one of the significant ways data analytics can impact healthcare staffing. Analyzing of historical data on patient admissions, seasonal trends, and even outbreaks of diseases, both healthcare and healthcare stuffing providers can forecast staffing needs well in advance and start sourcing candidates accordingly.\nKey Benefits:\nAnticipating Peaks: Predictive models (e.g., time series, random forest, etc.) can forecast of particular HCP\u0026rsquo;s also known as Healthcare Professionals (e.g., GNA/CNA, PN/LPN, RN\u0026rsquo;s, Therapist, Specialized Technician). This helps agencies and healthcare facilities avoid last-minute scrambling to fill shifts, leading to better patient care and reduced overtime costs. Optimizing Staffing Levels: Both overstaffing and understaffing are expensive mistakes. With accurate predictions both facilities and stuffing agencies can maintain the right balance and ensure they‚Äôre neither short on staff nor paying for unneeded shifts. For example, in my current role, I helped develop a Nurse Scoring System that used machine learning algorithms to predict the demand for nurses based on multiple variables such as seasonal trends, regional health statistics, and historical admission data. This system significantly optimized staffing decisions, reducing operational bottlenecks during peak times.\nEnhanced Candidate Matching and Screening\rOne of the issue I observed and faced working as a Data Analytics \u0026amp; Marketing Lead is matching candidates to job roles often relied on manual processes, which lead to delays and inefficiencies. Luckily I had the data\u0026rsquo;s and knew exactly how to use that on my advantage. Data analytics enables smarter candidate matching by using advanced algorithms to analyze both candidate qualifications and job requirements, ensuring a better fit.\nHow It Works:\nAutomated Screening: By analyzing data from resumes, certifications, and even past performance reviews, healthcare staffing companies can automatically filter through candidates to find the most qualified professionals quickly. Skills Matching: Using analytics to identify the specific skills and qualifications of healthcare workers allows staffing agencies to match candidates to the positions that suit their expertise, experience, and certifications. Cultural Fit Analytics: We can also utilize cultural fit assessments, this will ensure that candidates not only meet the technical requirements but also align with the healthcare facility‚Äôs values and work environment. This automated matching reduces the time spent manually screening candidates, freeing up HR teams to focus on more strategic tasks.\nOptimizing Recruitment with Cost-Per-Hire and Time-to-Hire Metrics\rRecruitment can be time-consuming and expensive, especially in the healthcare industry, where the need for a highly skilled workforce is critical. Data analytics can provide real-time insight into the costs of is created at each hire and at the time of employment, to enable staffing agencies to develop better recruitment strategies\nKey Performance Indicators (KPIs) to Track:\nCost-per-Hire: By analyzing recruitment channels (e.g., job boards, social media, referrals), agencies can identify cost-effective sources to recruit health professionals. This ensures that recruitment budgets are used effectively. Time-to-Hire: By analyzing the time spent at each stage of the recruitment process (from job posting to offer acceptance) helping staffing companies to identify bottlenecks and streamline the hiring pipeline. In my experience, reducing the time-to-hire was crucial when I worked on automating parts of the lead distribution process. By using real-time data to identify the most efficient recruitment sources, we cut down on hiring delays, ensuring that healthcare facilities were always well-staffed.\nImproved Compliance and Credentialing\rThe healthcare industry is heavily regulated, and ensuring that all personnel are properly licensed, certified, and trained is essential. Data analytics can streamline compliance and credentialing processes, ensuring that only the right personnel are employed in healthcare facilities.\nHow Data Helps:\nAutomated Credential Tracking: Analytics platforms can track the expiration dates of licenses and certifications, sending reminders when renewals are due. This reduces the risk of staffing non-compliant workers. Background Check Analytics: Data from background checks, previous employment history, and even patient feedback can be integrated into candidate profiles, providing a holistic view of each applicant‚Äôs qualifications. Through this, healthcare staffing companies can maintain compliance with state and federal regulations more effectively, ensuring higher-quality patient care and reducing legal risks.\nImproving Staff Retention with Predictive Analytics\rHigh turnover rates are a common challenge in healthcare staffing. Data analytics can help predict when staff members might leave, allowing agencies to take proactive measures to improve retention.\nPredictive Retention Models:\nIdentifying At-Risk Employees: By analyzing factors such as job satisfaction surveys, shift patterns, and overtime hours, staffing agencies can identify which employees are at risk of burnout or leaving their position. Retention Strategies: With this information, agencies can implement retention strategies such as flexible scheduling, additional training opportunities, or better incentives to retain valuable staff members. In healthcare, retaining skilled staff is crucial for maintaining high-quality patient care and reducing the costs associated with frequent rehiring.\nConclusion\rData analytics is reworking healthcare staffing to make it quicker, wiser, and even more effective. From predicting staffing desires to matching candidates and ensuring compliance, it supports organizations to work proactively while reducing costs and enhancing the quality of care.\nIn an enterprise where staffing decisions require the right professionals in proximity, leveraging statistics means that such decisions are even more accurate and effective, reaping benefits for both healthcare vendors and their patients.\n","date":"2024-10-14T00:00:00Z","image":"http://localhost:1313/p/data-helthcare/cover_hu1007085861240199004.jpg","permalink":"http://localhost:1313/p/data-helthcare/","title":"How Data Analytics Can Transform the Healthcare Staffing Industry"},{"content":"This interactive app, built with Streamlit and Facebook Prophet Model, allows users to analyze and forecast stock prices for popular companies like Microsoft (MSFT), Nvidia (NVDA), Google (GOOG), Apple (AAPL), and Tesla (TSLA) in real time. Users can easily select stocks, define a prediction period (1-5 years), and visualize historical data alongside forecasts.\nüîç Key Features:\nInteractive stock data visualization üìä\nAccurate price predictions using the Prophet model üìà\nDetailed analysis of forecast components\nUser-friendly interface\nView App\r","date":"2024-09-21T00:00:00Z","image":"http://localhost:1313/p/stock-predict/cover_hu6641854405770349223.jpeg","permalink":"http://localhost:1313/p/stock-predict/","title":"\"Stock Forecasting App\" - Strimlit \u0026 Facebook Prophet Model"},{"content":"\rFull View\rGoal of this project\rThe main purpose of this project is to explore the aftermath of Elon Musk's tweet on May 13, 2022, where he announced that Tesla would no longer accept Bitcoin as a form of payment. This unexpected decision sent shockwaves through the Bitcoin community and had a noticeable impact on its price.\rFor that, I employed the powerful statistical package called Google Causal Impact to study this phenomenon. With its ability to estimate the causal impact of an event on a target variable while accounting for other influencing factors, it was the perfect tool for this analysis.\nThe results were truly enlightening! The analysis revealed a significant and negative causal effect of Musk\u0026rsquo;s tweet on Bitcoin\u0026rsquo;s price. It became clear that the tweet had triggered panic within the Bitcoin community and influenced investor sentiment, leading to a noticeable decrease in price.\n\u003c!DOCTYPE html\u003e\rElon vs BTC\r1 Introduction¬∂\r1.1 Case Study¬∂On May 13, 2022, Elon Musk, the CEO of Tesla and a prominent figure in the cryptocurrency community, made a significant announcement on Twitter. In his tweet, Musk stated that Tesla would no longer accept Bitcoin as a form of payment. This unexpected decision sent shockwaves through the Bitcoin community and had a noticeable impact on the price of Bitcoin.\nTo analyze the causal relationship between Musk's tweet and Bitcoin's price and what would be the alternative sceinario of the Bitcoin prices if Elon Musk did not make the tweet, we will employ the \"Google Causal Impact\" using Python.\n1.2 Google Causal Impact¬∂The \"Google Causal Impact\" is a statistical approach used to estimate the causal impact of a particular event on a target variable, while accounting for various other factors that may influence the outcome. It is based on Bayesian structural time series models and is particularly effective in evaluating the impact of stock/crypto price change, marketing campaigns, policy changes, or other interventions. Google Causal Impact package was initially developed for \"R\" by Google team, later the packege was imported to \"python\"\nBy using this technique, we can isolate the effect of Elon Musk's tweet on Bitcoin's price, controlling for other factors such as market trends, trading volume, and external events. The analysis will provide insights into the direct influence of the tweet on Bitcoin's price movement, allowing us to understand the magnitude and significance of the impact. And help to visualize the what-if scenario for the Bitcoin price.\n2 Model Implementation¬∂\r2.1 Libraries, Dates, Data¬∂\nImport python libraries\rSet the data time-frame \u0026amp; stocks\rImport the stock datas\rIn¬†[1]:\rimport pandas as pd\rimport seaborn as sns\rimport matplotlib.pyplot as plt\rimport yfinance as yf\rfrom causalimpact import CausalImpact\rIn¬†[2]:\rtraining_start = \"2021-04-12\"\rtraining_end = \"2021-05-12\"\rtreatment_start = \"2021-05-13\"\rtreatment_end = \"2021-05-20\"\rend_stock = \"2021-05-21\"\rIn¬†[3]:\ry_stock = [\"BTC-USD\"]\ry = yf.download(y_stock,\rtraining_start,\rend_stock,\rinterval=\"1d\")\ry.head()\r[*********************100%***********************] 1 of 1 completed\rOut[3]:\rOpen\rHigh\rLow\rClose\rAdj Close\rVolume\rDate\r2021-04-12\r60175.945312\r61253.035156\r59589.875000\r59893.453125\r59893.453125\r51828688519\r2021-04-13\r59890.019531\r63742.285156\r59869.957031\r63503.457031\r63503.457031\r69983454362\r2021-04-14\r63523.753906\r64863.097656\r61554.796875\r63109.695312\r63109.695312\r77451779687\r2021-04-15\r63075.195312\r63821.671875\r62208.964844\r63314.011719\r63314.011719\r60954381579\r2021-04-16\r63258.503906\r63594.722656\r60222.531250\r61572.789062\r61572.789062\r84293007468\rIn¬†[4]:\ry = y[\"Adj Close\"].rename(\"BTC\")\ry.head(2)\rOut[4]:\rDate\r2021-04-12 59893.453125\r2021-04-13 63503.457031\rName: BTC, dtype: float64\rIn¬†[5]:\rX_stocks = [\"MSFT\", \"NVDA\", \"AI\", \"EA\", \"SQ\", \"CRSP\", \"GOOG\", \"WMT\"]\rX = yf.download(X_stocks,\rtraining_start,\rend_stock,\rinterval=\"1d\")\rX.head()\r[*********************100%***********************] 8 of 8 completed\rOut[5]:\rAdj Close\rClose\r...\rOpen\rVolume\rAI\rCRSP\rEA\rGOOG\rMSFT\rNVDA\rSQ\rWMT\rAI\rCRSP\r...\rSQ\rWMT\rAI\rCRSP\rEA\rGOOG\rMSFT\rNVDA\rSQ\rWMT\rDate\r2021-04-12\r59.889999\r114.389999\r139.680893\r112.739502\r250.827927\r151.826202\r265.200012\r134.979538\r59.889999\r114.389999\r...\r258.190002\r140.070007\r3111900\r1080900\r2058100\r31318000\r27148700\r86932400\r9051300\r6241000\r2021-04-13\r63.009998\r120.879997\r140.332626\r113.363503\r253.356720\r156.523056\r273.230011\r134.564331\r63.009998\r120.879997\r...\r267.480011\r139.800003\r3297300\r1352800\r2105300\r23310000\r23837500\r67621200\r10561700\r5799300\r2021-04-14\r68.459999\r123.110001\r139.838898\r112.741997\r250.514282\r152.505051\r258.399994\r134.516068\r68.459999\r123.110001\r...\r274.130005\r139.119995\r11721300\r1914000\r1424900\r20220000\r23070900\r38550000\r12486400\r7308200\r2021-04-15\r66.500000\r123.849998\r140.984390\r114.833000\r254.346634\r161.092590\r263.079987\r135.327118\r66.500000\r123.849998\r...\r262.850006\r139.399994\r6223600\r896300\r1654300\r27472000\r25627500\r59848000\r8944400\r7236800\r2021-04-16\r66.790001\r118.559998\r139.285889\r114.888000\r255.562057\r158.849014\r256.100006\r135.761581\r66.790001\r118.559998\r...\r263.239990\r140.889999\r2988000\r1359200\r2468400\r22596000\r24878600\r33520800\r8549300\r8829500\r5 rows √ó 48 columns\nüí° Here in confounders (X) stocks we are excluding stoks related to crypto currency \u0026amp; Elon Musk for remove selection biases\nIn¬†[6]:\rX = X[\"Adj Close\"]\rX.head(2)\rOut[6]:\rAI\rCRSP\rEA\rGOOG\rMSFT\rNVDA\rSQ\rWMT\rDate\r2021-04-12\r59.889999\r114.389999\r139.680893\r112.739502\r250.827927\r151.826202\r265.200012\r134.979538\r2021-04-13\r63.009998\r120.879997\r140.332626\r113.363503\r253.356720\r156.523056\r273.230011\r134.564331\rIn¬†[7]:\rX.index = X.index.tz_localize(None)\rIn¬†[8]:\rdf = pd.concat([y,X],axis=1).dropna()\rdf.head()\rOut[8]:\rBTC\rAI\rCRSP\rEA\rGOOG\rMSFT\rNVDA\rSQ\rWMT\rDate\r2021-04-12\r59893.453125\r59.889999\r114.389999\r139.680893\r112.739502\r250.827927\r151.826202\r265.200012\r134.979538\r2021-04-13\r63503.457031\r63.009998\r120.879997\r140.332626\r113.363503\r253.356720\r156.523056\r273.230011\r134.564331\r2021-04-14\r63109.695312\r68.459999\r123.110001\r139.838898\r112.741997\r250.514282\r152.505051\r258.399994\r134.516068\r2021-04-15\r63314.011719\r66.500000\r123.849998\r140.984390\r114.833000\r254.346634\r161.092590\r263.079987\r135.327118\r2021-04-16\r61572.789062\r66.790001\r118.559998\r139.285889\r114.888000\r255.562057\r158.849014\r256.100006\r135.761581\r2.2 Analysis¬∂\nCreate training dataframe\rApply \"Differencing\" technique to make the data stationery\rVisualize to check the correlation between \"Treatment\" and \"Confounders\"\rDropp least correlated confounders. Here the minimum threshold is 0.4 In¬†[9]:\rtraining_df = df[df.index \u0026lt;= training_end]\rtraining_df.head()\rOut[9]:\rBTC\rAI\rCRSP\rEA\rGOOG\rMSFT\rNVDA\rSQ\rWMT\rDate\r2021-04-12\r59893.453125\r59.889999\r114.389999\r139.680893\r112.739502\r250.827927\r151.826202\r265.200012\r134.979538\r2021-04-13\r63503.457031\r63.009998\r120.879997\r140.332626\r113.363503\r253.356720\r156.523056\r273.230011\r134.564331\r2021-04-14\r63109.695312\r68.459999\r123.110001\r139.838898\r112.741997\r250.514282\r152.505051\r258.399994\r134.516068\r2021-04-15\r63314.011719\r66.500000\r123.849998\r140.984390\r114.833000\r254.346634\r161.092590\r263.079987\r135.327118\r2021-04-16\r61572.789062\r66.790001\r118.559998\r139.285889\r114.888000\r255.562057\r158.849014\r256.100006\r135.761581\rIn¬†[10]:\rdifferencing = training_df.pct_change().dropna()\rdifferencing.head()\rOut[10]:\rBTC\rAI\rCRSP\rEA\rGOOG\rMSFT\rNVDA\rSQ\rWMT\rDate\r2021-04-13\r0.060274\r0.052095\r0.056736\r0.004666\r0.005535\r0.010082\r0.030936\r0.030279\r-0.003076\r2021-04-14\r-0.006201\r0.086494\r0.018448\r-0.003518\r-0.005482\r-0.011219\r-0.025670\r-0.054277\r-0.000359\r2021-04-15\r0.003237\r-0.028630\r0.006011\r0.008192\r0.018547\r0.015298\r0.056310\r0.018111\r0.006029\r2021-04-16\r-0.027501\r0.004361\r-0.042713\r-0.012047\r0.000479\r0.004779\r-0.013927\r-0.026532\r0.003210\r2021-04-19\r-0.094986\r-0.070220\r-0.029690\r-0.010067\r0.002019\r-0.007671\r-0.034611\r-0.042054\r-0.006400\rüí° In 90% of the cases time series datas are non stationary. So we are assuming this dataframe containing non stationary data\nIn¬†[11]:\rsns.heatmap(data=differencing.corr(),\rannot=True,\rfmt=\".1g\",\rcmap=\"Blues\")\rplt.show()\rIn¬†[12]:\rfinal_df = df.drop(columns=[\"EA\", \"GOOG\", \"WMT\"])\rfinal_df.head(2)\rOut[12]:\rBTC\rAI\rCRSP\rMSFT\rNVDA\rSQ\rDate\r2021-04-12\r59893.453125\r59.889999\r114.389999\r250.827927\r151.826202\r265.200012\r2021-04-13\r63503.457031\r63.009998\r120.879997\r253.356720\r156.523056\r273.230011\r2.3 Model Creation¬∂\nDefine Pre \u0026amp; Post period\rCreate the impact model\rSummarise the model\rIn¬†[13]:\rpre_period = [training_start,training_end]\rpost_period = [treatment_start, treatment_end]\rIn¬†[14]:\rimpact = CausalImpact(data = final_df,\rpre_period = pre_period,\rpost_period = post_period)\rsns.set_theme()\nimpact.plot(figsize=(23,14)) plt.show() C:\\Users\\Saiyan\\miniconda3\\envs\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\rself._init_dates(dates, freq)\rC:\\Users\\Saiyan\\miniconda3\\envs\\myenv\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:18: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score, iprint. The list of unsupported keyword arguments passed include: standardize, nseasons. After release 0.14, this will raise.\rwarnings.warn(\rC:\\Users\\Saiyan\\miniconda3\\envs\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['alpha']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\rwarnings.warn(msg, FutureWarning)\rC:\\Users\\Saiyan\\miniconda3\\envs\\myenv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\rself._init_dates(dates, freq)\rüí° Since we are working with stock datas, we will not consider the \"Cumelitive Effect\" and only conside the \"Relative Effect\"\nIn¬†[15]:\rprint(impact.summary())\rPosterior Inference {Causal Impact}\rAverage Cumulative\rActual 43971.47 263828.82\rPrediction (s.d.) 49543.16 (885.73) 297258.94 (5314.37)\r95% CI [47734.0, 51206.0] [286404.02, 307235.99]\rAbsolute effect (s.d.) -5571.69 (885.73) -33430.12 (5314.37) 95% CI [-7234.53, -3762.53][-43407.17, -22575.2]\nRelative effect (s.d.) -11.25% (1.79%) -11.25% (1.79%) 95% CI [-14.6%, -7.59%] [-14.6%, -7.59%]\nPosterior tail-area probability p: 0.0 Posterior prob. of a causal effect: 100.0%\nFor more details run the command: print(impact.summary(\u0026lsquo;report\u0026rsquo;)) 2.4 Interpretation¬∂The average value of the response variable during the post-intervention period was around 43971.47. If the intervention had not taken place, we would have expected an average response of 49543.17.The 95% interval for this expected value ranges from 47823.92 to 51362.75. That means if Elon Musk did not make that tweet the Bitcoin avarage price would be 49543.17 around that period, but because of the tweet the actual average Bitcoin price fall to 43971.47\nBy subtracting this expected value from the observed response, we estimate the causal effect of the intervention on the response variable to be -5571.7, with a 95% interval of -7391.28 to -3852.45. That indicates there is an negative effect on the Bitcoin price wich is around -5571.7\nIn relative terms, the response variable showed a decrease of approximately 11.25%. The 95% interval for this percentage change is from -14.92% to -7.78%. This indicates that the observed negative effect during the intervention period is statistically significant.\nThe probability of obtaining this effect by chance alone, as indicated by the Bayesian one-sided tail-area probability (p-value), is very small at 0.0. Therefore, the causal effect can be considered statistically significant.\n3 Conclusion¬∂By applying the Google Causal Impact we are able to prove there is a negative effect between the Elon Musk's Bitcoin tweet and the Bitcoin price fall. Understanding and analyzing causal relationships in cryptocurrency markets are crucial for investors, traders, policymakers, and researchers alike. By leveraging techniques like Google Causal Impact, we can gain a deeper understanding of the dynamics within these markets and make more informed decisions. This analysis serves as a reminder of the interconnectedness of social media, public figures, and the cryptocurrency landscape, and the potential implications of their interactions on market behavior.\nOverall, the case study showcases the power of data analysis techniques like Google Causal Impact in studying and quantifying the impact of specific events or interventions on cryptocurrency prices. This knowledge can provide valuable insights for market participants and contribute to a more comprehensive understanding of the complex and dynamic nature of the cryptocurrency market.\nCreated with Jupyter written by Sakiful Ahmed Saiyan.\r","date":"2023-05-27T00:00:00Z","image":"http://localhost:1313/p/elon-vs-btc/cover_hu16864638067430047759.jpg","permalink":"http://localhost:1313/p/elon-vs-btc/","title":"Elon Musk vs Bitcoin"},{"content":"GOAL\rThe goal of this Power BI dashboard is to provide a holistic view of the business by analyzing key metrics in four sections: Executive Summary, Store, Product, and Customer. It aims to offer high-level insights for executives, evaluate store performance, analyze product profitability, and understand customer behavior. By consolidating data in these areas, the dashboard facilitates informed decision-making and helps drive business growth.\nOUTCOME\rInformed decision-making for executives, resulting in strategic actions.\nImproved store performance through targeted optimizations.\nEnhanced product management based on profitability and customer preferences.\nCustomer-centric strategies driving loyalty and increased sales.\n","date":"2021-10-12T00:00:00Z","image":"http://localhost:1313/p/power-bi/cover_hu6324493936862104614.png","permalink":"http://localhost:1313/p/power-bi/","title":"Power BI Dashboard"}]